{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1\n",
    "max_length = 10\n",
    "results = np.zeros(shape=(len(samples),\n",
    "                            max_length,\n",
    "                            max(token_index.values()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1.\n",
    "        \n",
    "print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n",
      "Found 9 unique tokens.\n",
      "{'on': 4, 'ate': 7, 'mat': 5, 'dog': 6, 'cat': 2, 'the': 1, 'my': 8, 'homework': 9, 'sat': 3}\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding text from keras tokenizing\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.'] # can be multiple list's inside \n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "print sequences \n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_25/embedding_lookup/Identity:0\", shape=(?, 10, 8), dtype=float32)\n",
      "Tensor(\"flatten_25/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_25 (Embedding)     (None, 10, 8)             800       \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 80)                0         \n",
      "=================================================================\n",
      "Total params: 800\n",
      "Trainable params: 800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "body \n",
      "(2, 10)\n",
      "[[-0.01576035  0.00031934 -0.02959855 -0.04709757  0.01640693  0.02512174\n",
      "  -0.03977545 -0.02353597 -0.02589756 -0.00220241 -0.00373348 -0.03919834\n",
      "   0.02290286 -0.00738242 -0.01731491  0.02281094 -0.03819237  0.03567744\n",
      "   0.02641131  0.03006044 -0.04534373  0.01952783  0.01656697  0.00508574\n",
      "   0.01550351 -0.04327074  0.04419122 -0.0009838  -0.00480077 -0.04007524\n",
      "  -0.04153091  0.00309093 -0.01576035  0.00031934 -0.02959855 -0.04709757\n",
      "   0.01640693  0.02512174 -0.03977545 -0.02353597  0.00860562 -0.01981096\n",
      "   0.04684771  0.00194678 -0.04259693 -0.00274856  0.00936465  0.0448528\n",
      "  -0.04212893  0.02911488  0.0056726  -0.0369537   0.03821502 -0.03305522\n",
      "  -0.04406365 -0.02128718 -0.04212893  0.02911488  0.0056726  -0.0369537\n",
      "   0.03821502 -0.03305522 -0.04406365 -0.02128718 -0.04212893  0.02911488\n",
      "   0.0056726  -0.0369537   0.03821502 -0.03305522 -0.04406365 -0.02128718\n",
      "  -0.04212893  0.02911488  0.0056726  -0.0369537   0.03821502 -0.03305522\n",
      "  -0.04406365 -0.02128718]\n",
      " [-0.01576035  0.00031934 -0.02959855 -0.04709757  0.01640693  0.02512174\n",
      "  -0.03977545 -0.02353597  0.00765649  0.03872034  0.03024197 -0.04125527\n",
      "   0.01736119  0.01319852  0.04834732  0.03505306  0.03388727  0.02290044\n",
      "   0.00527088  0.01687989  0.03458593 -0.00952698 -0.04288037  0.03317011\n",
      "   0.0445604   0.03606541 -0.04406805 -0.00756714  0.02545083  0.00902446\n",
      "   0.03275431 -0.00533712  0.00463773  0.01605708  0.03730911  0.00695981\n",
      "  -0.04913794 -0.03241627  0.03829216 -0.01701421 -0.04212893  0.02911488\n",
      "   0.0056726  -0.0369537   0.03821502 -0.03305522 -0.04406365 -0.02128718\n",
      "  -0.04212893  0.02911488  0.0056726  -0.0369537   0.03821502 -0.03305522\n",
      "  -0.04406365 -0.02128718 -0.04212893  0.02911488  0.0056726  -0.0369537\n",
      "   0.03821502 -0.03305522 -0.04406365 -0.02128718 -0.04212893  0.02911488\n",
      "   0.0056726  -0.0369537   0.03821502 -0.03305522 -0.04406365 -0.02128718\n",
      "  -0.04212893  0.02911488  0.0056726  -0.0369537   0.03821502 -0.03305522\n",
      "  -0.04406365 -0.02128718]]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Input, Flatten\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "maxlen = 10 # words on vector\n",
    "vocab_size = 100 # words on vocabulary \n",
    "\n",
    "word_input = Input(shape=(maxlen,),dtype='float64')  \n",
    "\n",
    "# creating the embedding\n",
    "word_embedding = Embedding(input_dim=vocab_size,output_dim=8,input_length=maxlen)(word_input)\n",
    "print word_embedding\n",
    "\n",
    "word_vec = Flatten()(word_embedding) # flatten\n",
    "print word_vec\n",
    "embed_model = Model([word_input], word_vec) # combining all into a Keras model\n",
    "\n",
    "embed_model.compile(optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "                    loss='binary_crossentropy',metrics=['acc']) \n",
    "# compiling the model. parameters can be tuned as always.\n",
    "\n",
    "print embed_model.summary()\n",
    "\n",
    "# Encode data in one hot for testing encoding different\n",
    "coded_data = []\n",
    "for seq in samples:\n",
    "    coded_data.append(one_hot(seq, vocab_size))\n",
    "\n",
    "body = pad_sequences(sequences, maxlen=maxlen, padding='post', value=0.0) # fill with 0's to get same length\n",
    "print \"body \\n\", np.shape(body)\n",
    "\n",
    "\n",
    "embeddings = embed_model.predict(body) # finally getting the embeddings.\n",
    "print embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embedding \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "Se entiende por el proceso de mapear palabras u oraciones a palabras en enteros. De estos enteros se hacen vectores que se dividen por una relación semantica, siendo asi una manera de clasificar cada palabra en un vector diferente de acuerdo a la relación de esta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "URL fetch failure on https://s3.amazonaws.com/text-datasets/imdb.npz: None -- [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:726)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ba0f039697cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# loads as list of ints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/midulcehogar/Desktop/science/lib/python2.7/site-packages/keras/datasets/imdb.pyc\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path, num_words, skip_top, maxlen, seed, start_char, oov_char, index_from, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     path = get_file(path,\n\u001b[1;32m     56\u001b[0m                     \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'https://s3.amazonaws.com/text-datasets/imdb.npz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                     file_hash='599dadb1135973df5b59232a0e9a887c')\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/midulcehogar/Desktop/science/lib/python2.7/site-packages/keras/utils/data_utils.pyc\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: URL fetch failure on https://s3.amazonaws.com/text-datasets/imdb.npz: None -- [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:726)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 20 # cuts the review after this length of words\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) # loads as list of ints\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "\n",
    "# Turns the lists of integers into a 2D integer tensor of shape(samples, maxlen)                                 \n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen) \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "history = model.fit(x_train, y_train,\n",
    "                epochs=10,\n",
    "                batch_size=32,\n",
    "                validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with imdb dataset from scratch\n",
    "import os\n",
    "imdb_dir = '/Users/fchollet/Downloads/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname[-4:] == '.txt':\n",
    "                f = open(os.path.join(dir_name, fname))\n",
    "                texts.append(f.read())\n",
    "                f.close()\n",
    "                if label_type == 'neg':\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "                    \n",
    "# Tokenize data\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts\n",
    "                                         \n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "                                         \n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "                                         \n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "                                         \n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "                                         \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "                                         \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()                                         \n",
    "                                         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
