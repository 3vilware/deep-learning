{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1\n",
    "max_length = 10\n",
    "results = np.zeros(shape=(len(samples),\n",
    "                            max_length,\n",
    "                            max(token_index.values()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1.\n",
    "        \n",
    "print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n",
      "Found 9 unique tokens.\n",
      "{'on': 4, 'ate': 7, 'mat': 5, 'dog': 6, 'cat': 2, 'the': 1, 'my': 8, 'homework': 9, 'sat': 3}\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding text from keras tokenizing\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.'] # can be multiple list's inside \n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "print sequences \n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_25/embedding_lookup/Identity:0\", shape=(?, 10, 8), dtype=float32)\n",
      "Tensor(\"flatten_25/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_25 (Embedding)     (None, 10, 8)             800       \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 80)                0         \n",
      "=================================================================\n",
      "Total params: 800\n",
      "Trainable params: 800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "body \n",
      "(2, 10)\n",
      "[[-0.01576035  0.00031934 -0.02959855 -0.04709757  0.01640693  0.02512174\n",
      "  -0.03977545 -0.02353597 -0.02589756 -0.00220241 -0.00373348 -0.03919834\n",
      "   0.02290286 -0.00738242 -0.01731491  0.02281094 -0.03819237  0.03567744\n",
      "   0.02641131  0.03006044 -0.04534373  0.01952783  0.01656697  0.00508574\n",
      "   0.01550351 -0.04327074  0.04419122 -0.0009838  -0.00480077 -0.04007524\n",
      "  -0.04153091  0.00309093 -0.01576035  0.00031934 -0.02959855 -0.04709757\n",
      "   0.01640693  0.02512174 -0.03977545 -0.02353597  0.00860562 -0.01981096\n",
      "   0.04684771  0.00194678 -0.04259693 -0.00274856  0.00936465  0.0448528\n",
      "  -0.04212893  0.02911488  0.0056726  -0.0369537   0.03821502 -0.03305522\n",
      "  -0.04406365 -0.02128718 -0.04212893  0.02911488  0.0056726  -0.0369537\n",
      "   0.03821502 -0.03305522 -0.04406365 -0.02128718 -0.04212893  0.02911488\n",
      "   0.0056726  -0.0369537   0.03821502 -0.03305522 -0.04406365 -0.02128718\n",
      "  -0.04212893  0.02911488  0.0056726  -0.0369537   0.03821502 -0.03305522\n",
      "  -0.04406365 -0.02128718]\n",
      " [-0.01576035  0.00031934 -0.02959855 -0.04709757  0.01640693  0.02512174\n",
      "  -0.03977545 -0.02353597  0.00765649  0.03872034  0.03024197 -0.04125527\n",
      "   0.01736119  0.01319852  0.04834732  0.03505306  0.03388727  0.02290044\n",
      "   0.00527088  0.01687989  0.03458593 -0.00952698 -0.04288037  0.03317011\n",
      "   0.0445604   0.03606541 -0.04406805 -0.00756714  0.02545083  0.00902446\n",
      "   0.03275431 -0.00533712  0.00463773  0.01605708  0.03730911  0.00695981\n",
      "  -0.04913794 -0.03241627  0.03829216 -0.01701421 -0.04212893  0.02911488\n",
      "   0.0056726  -0.0369537   0.03821502 -0.03305522 -0.04406365 -0.02128718\n",
      "  -0.04212893  0.02911488  0.0056726  -0.0369537   0.03821502 -0.03305522\n",
      "  -0.04406365 -0.02128718 -0.04212893  0.02911488  0.0056726  -0.0369537\n",
      "   0.03821502 -0.03305522 -0.04406365 -0.02128718 -0.04212893  0.02911488\n",
      "   0.0056726  -0.0369537   0.03821502 -0.03305522 -0.04406365 -0.02128718\n",
      "  -0.04212893  0.02911488  0.0056726  -0.0369537   0.03821502 -0.03305522\n",
      "  -0.04406365 -0.02128718]]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Input, Flatten\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "maxlen = 10 # words on vector\n",
    "vocab_size = 100 # words on vocabulary \n",
    "\n",
    "word_input = Input(shape=(maxlen,),dtype='float64')  \n",
    "\n",
    "# creating the embedding\n",
    "word_embedding = Embedding(input_dim=vocab_size,output_dim=8,input_length=maxlen)(word_input)\n",
    "print word_embedding\n",
    "\n",
    "word_vec = Flatten()(word_embedding) # flatten\n",
    "print word_vec\n",
    "embed_model = Model([word_input], word_vec) # combining all into a Keras model\n",
    "\n",
    "embed_model.compile(optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "                    loss='binary_crossentropy',metrics=['acc']) \n",
    "# compiling the model. parameters can be tuned as always.\n",
    "\n",
    "print embed_model.summary()\n",
    "\n",
    "# Encode data in one hot for testing encoding different\n",
    "coded_data = []\n",
    "for seq in samples:\n",
    "    coded_data.append(one_hot(seq, vocab_size))\n",
    "\n",
    "body = pad_sequences(sequences, maxlen=maxlen, padding='post', value=0.0) # fill with 0's to get same length\n",
    "print \"body \\n\", np.shape(body)\n",
    "\n",
    "\n",
    "embeddings = embed_model.predict(body) # finally getting the embeddings.\n",
    "print embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embedding \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "Se entiende por el proceso de mapear palabras u oraciones a palabras en enteros. De estos enteros se hacen vectores que se dividen por una relación semantica, siendo asi una manera de clasificar cada palabra en un vector diferente de acuerdo a la relación de esta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded...\n",
      "\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] (25000,)\n",
      "\n",
      "Padded:\n",
      "65 (25000, 20)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 1s 54us/step - loss: 0.6759 - acc: 0.6047 - val_loss: 0.6398 - val_acc: 0.6806\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.5658 - acc: 0.7427 - val_loss: 0.5467 - val_acc: 0.7204\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 0.4752 - acc: 0.7808 - val_loss: 0.5113 - val_acc: 0.7384\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 1s 40us/step - loss: 0.4264 - acc: 0.8077 - val_loss: 0.5008 - val_acc: 0.7454\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 1s 52us/step - loss: 0.3931 - acc: 0.8256 - val_loss: 0.4981 - val_acc: 0.7536\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 0.3668 - acc: 0.8395 - val_loss: 0.5014 - val_acc: 0.7528\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 0.3435 - acc: 0.8533 - val_loss: 0.5052 - val_acc: 0.7518\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 1s 55us/step - loss: 0.3223 - acc: 0.8657 - val_loss: 0.5132 - val_acc: 0.7486\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 1s 46us/step - loss: 0.3023 - acc: 0.8764 - val_loss: 0.5213 - val_acc: 0.7490\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 0.2840 - acc: 0.8860 - val_loss: 0.5302 - val_acc: 0.7466\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 20 # cuts the review after this length of words\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) # loads as list of ints\n",
    "print \"Data loaded...\\n\"\n",
    "print x_train[0], x_train.shape\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "print \"\\nPadded:\\n\", x_train[0][0], x_train.shape\n",
    "\n",
    "# Turns the lists of integers into a 2D integer tensor of shape(samples, maxlen)                                 \n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen) \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "history = model.fit(x_train, y_train,\n",
    "                epochs=10,\n",
    "                batch_size=32,\n",
    "                validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with imdb dataset from scratch\n",
    "import os\n",
    "imdb_dir = '/Users/fchollet/Downloads/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname[-4:] == '.txt':\n",
    "                f = open(os.path.join(dir_name, fname))\n",
    "                texts.append(f.read())\n",
    "                f.close()\n",
    "                if label_type == 'neg':\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "                    \n",
    "# Tokenize data\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts\n",
    "                                         \n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "                                         \n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "                                         \n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "                                         \n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "                                         \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "                                         \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()                                         \n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
